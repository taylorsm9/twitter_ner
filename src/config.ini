
[MODEL]
# Model type
# Values accepted:
# 0: BERT
# 1: BERTweet
MODEL_TYPE = 1

[DATA]
# Dataset
# Values accepted:
# 0: BERT
# 1: BERTweet
DATASET = 1

[TRAINING]
MAX_LEN = 128
# A larger batch size will make training go faster, but too large will use all
# available memory and throw an error Trial and error will be needed to find an
# optimal batch size depending on hardware. A batch size of 6 was sufficient to
# complete epochs in under 2 minutes on a laptop, a batch size of 32 may
# take up to 60 minutes per epoch on colab.

TRAIN_BATCH_SIZE = 12
TEST_BATCH_SIZE = 12

# More Epochs can improve performance. 
# In our implementation, we use a dropout layer to stop training if we do not see enough improvement
# Because of this, this setting can be made arbitrarily high. We did not see a model train for more than 23 epochs. 
EPOCHS = 10

#Path to save trained models
MODEL_PATH = twitter_ner/models/
